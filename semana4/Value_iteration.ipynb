{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFH_bfp6jUEQ"
      },
      "outputs": [],
      "source": [
        "import mdp, util\n",
        "\n",
        "from learningAgents import ValueEstimationAgent\n",
        "import collections\n",
        "\n",
        "class ValueIterationAgent(ValueEstimationAgent):\n",
        "    \"\"\"\n",
        "        * Please read learningAgents.py before reading this.*\n",
        "\n",
        "        A ValueIterationAgent takes a Markov decision process\n",
        "        (see mdp.py) on initialization and runs value iteration\n",
        "        for a given number of iterations using the supplied\n",
        "        discount factor.\n",
        "    \"\"\"\n",
        "    def __init__(self, mdp, discount = 0.9, iterations = 100):\n",
        "        \"\"\"\n",
        "          Your value iteration agent should take an mdp on\n",
        "          construction, run the indicated number of iterations\n",
        "          and then act according to the resulting policy.\n",
        "\n",
        "          Some useful mdp methods you will use:\n",
        "              mdp.getStates()\n",
        "              mdp.getPossibleActions(state)\n",
        "              mdp.getTransitionStatesAndProbs(state, action)\n",
        "              mdp.getReward(state, action, nextState)\n",
        "              mdp.isTerminal(state)\n",
        "        \"\"\"\n",
        "        self.mdp = mdp\n",
        "        self.discount = discount\n",
        "        self.iterations = iterations\n",
        "        self.values = util.Counter() # A Counter is a dict with default 0\n",
        "        self.runValueIteration()\n",
        "\n",
        "    def runValueIteration(self):\n",
        "        # Write value iteration code here\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "        for iteration in range(self.iterations):\n",
        "            temp = util.Counter()\n",
        "            for state in self.mdp.getStates():\n",
        "                #the value for terminal state is 0\n",
        "                if self.mdp.isTerminal(state):\n",
        "                    temp[state] = 0\n",
        "                else:\n",
        "                    #get actions and rewards\n",
        "                    maximumValue = -99999\n",
        "                    #actions for the state\n",
        "                    actions = self.mdp.getPossibleActions(state)\n",
        "                    for action in actions:\n",
        "                        #find state and probability for the transition associated\n",
        "                        #with actions\n",
        "                        t = self.mdp.getTransitionStatesAndProbs(state, action)\n",
        "                        value = 0\n",
        "                        for stateAndProb in t:\n",
        "                            value += stateAndProb[1] * (self.mdp.getReward(state, action, stateAndProb[1]) \\\n",
        "                            + self.discount * self.values[stateAndProb[0]])\n",
        "                        maximumValue = max(value, maximumValue)\n",
        "                    if maximumValue != -99999:\n",
        "                        temp[state] = maximumValue\n",
        "            self.values = temp\n",
        "\n",
        "\n",
        "    def getValue(self, state):\n",
        "        \"\"\"\n",
        "          Return the value of the state (computed in __init__).\n",
        "        \"\"\"\n",
        "        return self.values[state]\n",
        "\n",
        "    def getPolicy(self, state):\n",
        "        return self.computeActionFromValues(state)\n",
        "\n",
        "    def getAction(self, state):\n",
        "        \"Returns the policy at the state (no exploration).\"\n",
        "        return self.computeActionFromValues(state)\n",
        "\n",
        "class AsynchronousValueIterationAgent(ValueIterationAgent):\n",
        "    \"\"\"\n",
        "        * Please read learningAgents.py before reading this.*\n",
        "\n",
        "        An AsynchronousValueIterationAgent takes a Markov decision process\n",
        "        (see mdp.py) on initialization and runs cyclic value iteration\n",
        "        for a given number of iterations using the supplied\n",
        "        discount factor.\n",
        "    \"\"\"\n",
        "    def __init__(self, mdp, discount = 0.9, iterations = 1000):\n",
        "        \"\"\"\n",
        "          Your cyclic value iteration agent should take an mdp on\n",
        "          construction, run the indicated number of iterations,\n",
        "          and then act according to the resulting policy. Each iteration\n",
        "          updates the value of only one state, which cycles through\n",
        "          the states list. If the chosen state is terminal, nothing\n",
        "          happens in that iteration.\n",
        "\n",
        "          Some useful mdp methods you will use:\n",
        "              mdp.getStates()\n",
        "              mdp.getPossibleActions(state)\n",
        "              mdp.getTransitionStatesAndProbs(state, action)\n",
        "              mdp.getReward(state)\n",
        "              mdp.isTerminal(state)\n",
        "        \"\"\"\n",
        "        ValueIterationAgent.__init__(self, mdp, discount, iterations)\n",
        "\n",
        "    def runValueIteration(self):\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "\n",
        "        states = self.mdp.getStates()\n",
        "\n",
        "        for iteration in range(self.iterations):\n",
        "            state  = states[iteration % len(states)]\n",
        "            if not self.mdp.isTerminal(state):\n",
        "                actions = self.mdp.getPossibleActions(state)\n",
        "                maximumValue = max([self.getQValue(state,action) for action in actions])\n",
        "                self.values[state] = maximumValue\n",
        "\n",
        "\n",
        "class PrioritizedSweepingValueIterationAgent(AsynchronousValueIterationAgent):\n",
        "    \"\"\"\n",
        "        * Please read learningAgents.py before reading this.*\n",
        "\n",
        "        A PrioritizedSweepingValueIterationAgent takes a Markov decision process\n",
        "        (see mdp.py) on initialization and runs prioritized sweeping value iteration\n",
        "        for a given number of iterations using the supplied parameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, mdp, discount = 0.9, iterations = 100, theta = 1e-5):\n",
        "        \"\"\"\n",
        "          Your prioritized sweeping value iteration agent should take an mdp on\n",
        "          construction, run the indicated number of iterations,\n",
        "          and then act according to the resulting policy.\n",
        "        \"\"\"\n",
        "        self.theta = theta\n",
        "        ValueIterationAgent.__init__(self, mdp, discount, iterations)\n",
        "\n",
        "    def runValueIteration(self):\n",
        "        \"*** YOUR CODE HERE ***\"\n",
        "\n",
        "        pQueue = util.PriorityQueue()\n",
        "\n",
        "        predecessors = {}\n",
        "\n",
        "        #fid all predecessors\n",
        "        for state in self.mdp.getStates():\n",
        "            if not self.mdp.isTerminal(state):\n",
        "                for action in self.mdp.getPossibleActions(state):\n",
        "                    for stateAndProb in self.mdp.getTransitionStatesAndProbs(state, action):\n",
        "                        if stateAndProb[0] in predecessors:\n",
        "                            predecessors[stateAndProb[0]].add(state)\n",
        "                        else:\n",
        "                            predecessors[stateAndProb[0]] = {state}\n",
        "\n",
        "\n",
        "        for state in self.mdp.getStates():\n",
        "            if not self.mdp.isTerminal(state):\n",
        "                diff = abs(self.values[state] - max([ \\\n",
        "                self.computeQValueFromValues(state, action) for action in \\\n",
        "                self.mdp.getPossibleActions(state) ]) )\n",
        "                #pushing negative into queue\n",
        "                pQueue.update(state, -diff)\n",
        "\n",
        "        for iteration in range(self.iterations):\n",
        "            if pQueue.isEmpty():\n",
        "                break\n",
        "            state = pQueue.pop()\n",
        "            if not self.mdp.isTerminal(state):\n",
        "                self.values[state] = max([self.computeQValueFromValues(state, action)\\\n",
        "                 for action in self.mdp.getPossibleActions(state)])\n",
        "\n",
        "            for p in predecessors[state]:\n",
        "                if not self.mdp.isTerminal(p):\n",
        "                    diff = abs(self.values[p] - max([self.computeQValueFromValues(p, action)\\\n",
        "                     for action in self.mdp.getPossibleActions(p)]))\n",
        "\n",
        "                    if diff > self.theta:\n",
        "                            pQueue.update(p, -diff)\n"
      ]
    }
  ]
}