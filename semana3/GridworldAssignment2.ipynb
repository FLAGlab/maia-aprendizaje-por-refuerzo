{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "placed-institute",
   "metadata": {},
   "source": [
    "## GridWorld con MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thirty-fellow",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mdp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ebb0c38bb8f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmdp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mdp'"
     ]
    }
   ],
   "source": [
    "#Importación de librerias...\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import mdp\n",
    "import environment\n",
    "import util\n",
    "import optparse\n",
    "\n",
    "#Creación de la clase Gridworld, usando procesos de Markov\n",
    "class Gridworld(mdp.MarkovDecisionProcess):\n",
    "    def __init__(self, grid):\n",
    "        # layout\n",
    "        if type(grid) == type([]): grid = makeBoard(grid)\n",
    "        self.grid = grid\n",
    "        self.livingReward = 0.0\n",
    "        self.noise = 0.2\n",
    "\n",
    "#Creación del ambiente        \n",
    "class Environment:\n",
    "    def __init__(self, grid):\n",
    "        # layout\n",
    "        if type(grid) == type([]): grid = makeBoard(grid)\n",
    "        self.grid = grid\n",
    "        self.current = (0,0)\n",
    "\n",
    "    #Definición del metodo para el retorno del estado (casilla) actual del agente.        \n",
    "    def get_current_state(self):\n",
    "        return self.current_state\n",
    "\n",
    "    #Definición del metodo para el retorno de las acciones disponibles para cada estado, evaluando la tupla (i,j)\n",
    "    def get_possible_actions(self, state):\n",
    "        actions = ()\n",
    "        if state[0] > 0:\n",
    "          actions += ('up', ) #Arriba\n",
    "        if state[0] < self.dimensions -1:\n",
    "          actions += ('down', ) #Abajo\n",
    "        if state[1] > 0:\n",
    "          actions += ('left', ) #Izquierda\n",
    "        if state[1] < self.dimensions -1:\n",
    "          actions += ('right', ) #Derecha\n",
    "        return actions\n",
    "\n",
    "    #Metodo que recibe la acción a ejecutar. Retorna el nuevo estado del agente y el valor de la recompensa obtenida\n",
    "    def do_action(self, action):\n",
    "        if action == 'north':\n",
    "          self.current_state[0] -= 1\n",
    "        elif action == 'south':\n",
    "          self.current_state[0] += 1\n",
    "        elif action == 'left':\n",
    "          self.current_state[1] -= 1\n",
    "        elif action == 'right':\n",
    "          self.current_state[1] += 1\n",
    "        reward = self.rewards[self.current_state[0]][self.current_state[1]]\n",
    "        return (reward, self.current_state)\n",
    "    \n",
    "    #Metodo de restablecimiento a estado inicial.\n",
    "    def reset(self):\n",
    "        self.current_state = (0,0)\n",
    "\n",
    "    #Metodo para el determinar el final o no de la ejecución del juego. \n",
    "    def is_terminal(self):\n",
    "        state = self.get_current_state()\n",
    "        reward = self.rewards[state[0]][state[1]]\n",
    "        return reward == 1    \n",
    "\n",
    "#Creación de la clase Board     \n",
    "class Board:\n",
    "    \n",
    "    #Definición de las dimensiones del tablero\n",
    "    def __init__(self, width, height, initialValue=' '):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.data = [[initialValue for y in range(height)] for x in range(width)]\n",
    "        self.terminalState = 'TERMINAL_STATE'\n",
    "\n",
    "#Construcción del tablero segun dimensiones.        \n",
    "def makeBoard(boardString):\n",
    "    width, height = len(boardString[0]), len(boardString)\n",
    "    grid = (width, height)\n",
    "    for ybar, line in enumerate(boardString):\n",
    "        y = height - ybar - 1\n",
    "        for x, el in enumerate(line):\n",
    "            grid[x][y] = el\n",
    "    return grid\n",
    "\n",
    "#caracteristicas del tablero (S=punto de partida, +1 Recompensa positiva, -1 Recompensa negativa\n",
    "#' ' por defecto = 0.0, # = obstaculos) \n",
    "def board():\n",
    "        \n",
    "    grid = [['S',' ',' ',' ',' ',' ',' ',' ',' ',' ',],\n",
    "            [' ',' ',' ',' ',' ',' ',' ',' ',' ',' ',],\n",
    "            [' ','#','#','#','#',' ','#','#','#',' ',],\n",
    "            [' ',' ',' ',' ','#',' ',' ',' ',' ',' ',],\n",
    "            [' ',' ',' ',' ','#', -1,' ',' ',' ',' ',],\n",
    "            [' ',' ',' ',' ','#', +1,' ',' ',' ',' ',],\n",
    "            [' ',' ',' ',' ','#',' ',' ',' ',' ',' ',],\n",
    "            [' ',' ',' ',' ','#', -1, -1,' ',' ',' ',],\n",
    "            [' ',' ',' ',' ',' ',' ',' ',' ',' ',' ',],\n",
    "            [' ',' ',' ',' ',' ',' ',' ',' ',' ',' ']]\n",
    "    return Environment(grid)\n",
    "\n",
    "#Acciones tomadas desde el usuario (manualmente)\n",
    "def getUserAction(state, actionFunction):\n",
    "    import graphicsUtils\n",
    "    action = None\n",
    "    while True:\n",
    "        keys = graphicsUtils.wait_for_keys()\n",
    "        if 'Up' in keys: action = 'north'\n",
    "        if 'Down' in keys: action = 'south'\n",
    "        if 'Left' in keys: action = 'west'\n",
    "        if 'Right' in keys: action = 'east'\n",
    "        if 'q' in keys: sys.exit(0)\n",
    "        if action == None: continue\n",
    "        break\n",
    "    actions = actionFunction(state)\n",
    "    if action not in actions:\n",
    "        action = actions[0]\n",
    "    return action\n",
    "\n",
    "def printString(x): print(x)\n",
    "\n",
    "    #####################################################################################\n",
    "    ##                                                                                 ## \n",
    "    ## Licensing Information:  You are free to use or extend these projects for        ##\n",
    "    ## educational purposes provided that (1) you do not distribute or publish         ##\n",
    "    ## solutions, (2) you retain this notice, and (3) you provide clear                ##\n",
    "    ## attribution to UC Berkeley, including a link to http://ai.berkeley.edu.         ##\n",
    "    ##                                                                                 ##\n",
    "    ## Attribution Information: The Pacman AI projects were developed at UC Berkeley.  ##\n",
    "    ## The core projects and autograders were primarily created by John DeNero         ##\n",
    "    ## (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).                 ##\n",
    "    ## Student side autograding was added by Brad Miller, Nick Hay, and                ##\n",
    "    ## Pieter Abbeel (pabbeel@cs.berkeley.edu).                                        ##\n",
    "    ##                                                                                 ##\n",
    "    #####################################################################################\n",
    "\n",
    "#definición de la ejecución por episodio.    \n",
    "def runEpisode(agent, environment, discount, decision, display, message, pause, episode):\n",
    "    returns = 0\n",
    "    totalDiscount = 1.0\n",
    "    environment.reset()\n",
    "    if 'startEpisode' in dir(agent): agent.startEpisode()\n",
    "    message(\"BEGINNING EPISODE: \"+str(episode)+\"\\n\")\n",
    "    while True:\n",
    "\n",
    "        # DISPLAY CURRENT STATE\n",
    "        state = environment.getCurrentState()\n",
    "        display(state)\n",
    "        pause()\n",
    "\n",
    "        # END IF IN A TERMINAL STATE\n",
    "        actions = environment.getPossibleActions(state)\n",
    "        if len(actions) == 0:\n",
    "            message(\"EPISODE \"+str(episode)+\" COMPLETE: RETURN WAS \"+str(returns)+\"\\n\")\n",
    "            return returns\n",
    "\n",
    "        # GET ACTION (USUALLY FROM AGENT)\n",
    "        action = decision(state)\n",
    "        if action == None:\n",
    "            raise 'Error: Agent returned None action'\n",
    "\n",
    "        # EXECUTE ACTION\n",
    "        nextState, reward = environment.doAction(action)\n",
    "        message(\"Started in state: \"+str(state)+\n",
    "                \"\\nTook action: \"+str(action)+\n",
    "                \"\\nEnded in state: \"+str(nextState)+\n",
    "                \"\\nGot reward: \"+str(reward)+\"\\n\")\n",
    "        # UPDATE LEARNER\n",
    "        if 'observeTransition' in dir(agent):\n",
    "            agent.observeTransition(state, action, nextState, reward)\n",
    "\n",
    "        returns += reward * totalDiscount\n",
    "        totalDiscount *= discount\n",
    "\n",
    "    if 'stopEpisode' in dir(agent):\n",
    "        agent.stopEpisode()\n",
    "\n",
    "def parseOptions():\n",
    "    optParser = optparse.OptionParser()\n",
    "    optParser.add_option('-d', '--discount',action='store',\n",
    "                         type='float',dest='discount',default=0.9,\n",
    "                         help='Discount on future (default %default)')\n",
    "    optParser.add_option('-r', '--livingReward',action='store',\n",
    "                         type='float',dest='livingReward',default=0.0,\n",
    "                         metavar=\"R\", help='Reward for living for a time step (default %default)')\n",
    "    optParser.add_option('-n', '--noise',action='store',\n",
    "                         type='float',dest='noise',default=0.2,\n",
    "                         metavar=\"P\", help='How often action results in ' +\n",
    "                         'unintended direction (default %default)' )\n",
    "    optParser.add_option('-e', '--epsilon',action='store',\n",
    "                         type='float',dest='epsilon',default=0.3,\n",
    "                         metavar=\"E\", help='Chance of taking a random action in q-learning (default %default)')\n",
    "    optParser.add_option('-l', '--learningRate',action='store',\n",
    "                         type='float',dest='learningRate',default=0.5,\n",
    "                         metavar=\"P\", help='TD learning rate (default %default)' )\n",
    "    optParser.add_option('-i', '--iterations',action='store',\n",
    "                         type='int',dest='iters',default=10,\n",
    "                         metavar=\"K\", help='Number of rounds of value iteration (default %default)')\n",
    "    optParser.add_option('-k', '--episodes',action='store',\n",
    "                         type='int',dest='episodes',default=1,\n",
    "                         metavar=\"K\", help='Number of epsiodes of the MDP to run (default %default)')\n",
    "    optParser.add_option('-g', '--grid',action='store',\n",
    "                         metavar=\"G\", type='string',dest='grid',default=\"AssignmentGrid\",\n",
    "                         help='Grid to use (case sensitive; options are AssignmentGrid BookGrid, BridgeGrid, CliffGrid, MazeGrid, default %default)' )\n",
    "    optParser.add_option('-w', '--windowSize', metavar=\"X\", type='int',dest='gridSize',default=70,\n",
    "                         help='Request a window width of X pixels *per grid cell* (default %default)')\n",
    "    optParser.add_option('-a', '--agent',action='store', metavar=\"A\",\n",
    "                         type='string',dest='agent',default=\"random\",\n",
    "                         help='Agent type (options are \\'random\\', \\'value\\' and \\'q\\', default %default)')\n",
    "    optParser.add_option('-t', '--text',action='store_true',\n",
    "                         dest='textDisplay',default=False,\n",
    "                         help='Use text-only ASCII display')\n",
    "    optParser.add_option('-p', '--pause',action='store_true',\n",
    "                         dest='pause',default=False,\n",
    "                         help='Pause GUI after each time step when running the MDP')\n",
    "    optParser.add_option('-q', '--quiet',action='store_true',\n",
    "                         dest='quiet',default=False,\n",
    "                         help='Skip display of any learning episodes')\n",
    "    optParser.add_option('-s', '--speed',action='store', metavar=\"S\", type=float,\n",
    "                         dest='speed',default=1.0,\n",
    "                         help='Speed of animation, S > 1.0 is faster, 0.0 < S < 1.0 is slower (default %default)')\n",
    "    optParser.add_option('-m', '--manual',action='store_true',\n",
    "                         dest='manual',default=False,\n",
    "                         help='Manually control agent')\n",
    "    optParser.add_option('-v', '--valueSteps',action='store_true' ,default=False,\n",
    "                         help='Display each step of value iteration')\n",
    "\n",
    "    opts, args = optParser.parse_args()\n",
    "\n",
    "    if opts.manual and opts.agent != 'q':\n",
    "        print('## Disabling Agents in Manual Mode (-m) ##')\n",
    "        opts.agent = None\n",
    "\n",
    "    # MANAGE CONFLICTS\n",
    "    if opts.textDisplay or opts.quiet:\n",
    "    # if opts.quiet:\n",
    "        opts.pause = False\n",
    "        # opts.manual = False\n",
    "\n",
    "    if opts.manual:\n",
    "        opts.pause = True\n",
    "\n",
    "    return opts\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    opts = parseOptions()\n",
    "\n",
    "    ###########################\n",
    "    # GET THE GRIDWORLD\n",
    "    ###########################\n",
    "\n",
    "    import gridworld\n",
    "    mdpFunction = getattr(gridworld, \"get\"+opts.grid)\n",
    "    mdp = mdpFunction()\n",
    "    mdp.setLivingReward(opts.livingReward)\n",
    "    mdp.setNoise(opts.noise)\n",
    "    env = gridworld.GridworldEnvironment(mdp)\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    # GET THE DISPLAY ADAPTER\n",
    "    ###########################\n",
    "\n",
    "    import textGridworldDisplay\n",
    "    display = textGridworldDisplay.TextGridworldDisplay(mdp)\n",
    "    if not opts.textDisplay:\n",
    "        import graphicsGridworldDisplay\n",
    "        display = graphicsGridworldDisplay.GraphicsGridworldDisplay(mdp, opts.gridSize, opts.speed)\n",
    "    try:\n",
    "        display.start()\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit(0)\n",
    "\n",
    "    ###########################\n",
    "    # GET THE AGENT\n",
    "    ###########################\n",
    "\n",
    "    import valueIterationAgents, qlearningAgents\n",
    "    a = None\n",
    "    if opts.agent == 'value':\n",
    "        a = valueIterationAgents.ValueIterationAgent(mdp, opts.discount, opts.iters)\n",
    "    elif opts.agent == 'q':\n",
    "        env.getPossibleActions, opts.discount, opts.learningRate, opts.epsilon\n",
    "        #simulationFn = lambda agent, state: simulation.GridworldSimulation(agent,state,mdp)\n",
    "        gridWorldEnv = Environment(mdp)\n",
    "        actionFn = lambda state: mdp.getPossibleActions(state)\n",
    "        qLearnOpts = {'gamma': opts.discount,\n",
    "                      'alpha': opts.learningRate,\n",
    "                      'epsilon': opts.epsilon,\n",
    "                      'actionFn': actionFn}\n",
    "        a = qlearningAgents.QLearningAgent(**qLearnOpts)\n",
    "    elif opts.agent == 'random':\n",
    "        # # No reason to use the random agent without episodes\n",
    "        if opts.episodes == 0:\n",
    "            opts.episodes = 10\n",
    "        class RandomAgent:\n",
    "            def getAction(self, state):\n",
    "                return random.choice(mdp.getPossibleActions(state))\n",
    "            def getValue(self, state):\n",
    "                return 0.0\n",
    "            def getQValue(self, state, action):\n",
    "                return 0.0\n",
    "            def getPolicy(self, state):\n",
    "                \"NOTE: 'random' is a special policy value; don't use it in your code.\"\n",
    "                return 'random'\n",
    "            def update(self, state, action, nextState, reward):\n",
    "                pass\n",
    "        a = RandomAgent()\n",
    "    elif opts.agent == 'asynchvalue':\n",
    "        a = valueIterationAgents.AsynchronousValueIterationAgent(mdp, opts.discount, opts.iters)\n",
    "    elif opts.agent == 'priosweepvalue':\n",
    "        a = valueIterationAgents.PrioritizedSweepingValueIterationAgent(mdp, opts.discount, opts.iters)\n",
    "    else:\n",
    "        if not opts.manual: raise Exception('Unknown agent type: '+opts.agent)\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    # RUN EPISODES\n",
    "    ###########################\n",
    "    # DISPLAY Q/V VALUES BEFORE SIMULATION OF EPISODES\n",
    "    try:\n",
    "        if not opts.manual and opts.agent in ('value', 'asynchvalue', 'priosweepvalue'):\n",
    "            if opts.valueSteps:\n",
    "                for i in range(opts.iters):\n",
    "                    tempAgent = valueIterationAgents.ValueIterationAgent(mdp, opts.discount, i)\n",
    "                    display.displayValues(tempAgent, message = \"VALUES AFTER \"+str(i)+\" ITERATIONS\")\n",
    "                    display.pause()\n",
    "\n",
    "            display.displayValues(a, message = \"VALUES AFTER \"+str(opts.iters)+\" ITERATIONS\")\n",
    "            display.pause()\n",
    "            display.displayQValues(a, message = \"Q-VALUES AFTER \"+str(opts.iters)+\" ITERATIONS\")\n",
    "            display.pause()\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit(0)\n",
    "\n",
    "\n",
    "\n",
    "    # FIGURE OUT WHAT TO DISPLAY EACH TIME STEP (IF ANYTHING)\n",
    "    displayCallback = lambda x: None\n",
    "    if not opts.quiet:\n",
    "        if opts.manual and opts.agent == None:\n",
    "            displayCallback = lambda state: display.displayNullValues(state)\n",
    "        else:\n",
    "            if opts.agent in ('random', 'value', 'asynchvalue', 'priosweepvalue'):\n",
    "                displayCallback = lambda state: display.displayValues(a, state, \"CURRENT VALUES\")\n",
    "            if opts.agent == 'q': displayCallback = lambda state: display.displayQValues(a, state, \"CURRENT Q-VALUES\")\n",
    "\n",
    "    messageCallback = lambda x: printString(x)\n",
    "    if opts.quiet:\n",
    "        messageCallback = lambda x: None\n",
    "\n",
    "    # FIGURE OUT WHETHER TO WAIT FOR A KEY PRESS AFTER EACH TIME STEP\n",
    "    pauseCallback = lambda : None\n",
    "    if opts.pause:\n",
    "        pauseCallback = lambda : display.pause()\n",
    "\n",
    "    # FIGURE OUT WHETHER THE USER WANTS MANUAL CONTROL (FOR DEBUGGING AND DEMOS)\n",
    "    if opts.manual:\n",
    "        decisionCallback = lambda state : getUserAction(state, mdp.getPossibleActions)\n",
    "    else:\n",
    "        decisionCallback = a.getAction\n",
    "\n",
    "    # RUN EPISODES\n",
    "    if opts.episodes > 0:\n",
    "        print()\n",
    "        print(\"RUNNING\", opts.episodes, \"EPISODES\")\n",
    "        print()\n",
    "    returns = 0\n",
    "    for episode in range(1, opts.episodes+1):\n",
    "        returns += runEpisode(a, env, opts.discount, decisionCallback, displayCallback, messageCallback, pauseCallback, episode)\n",
    "    if opts.episodes > 0:\n",
    "        print()\n",
    "        print(\"AVERAGE RETURNS FROM START STATE: \"+str((returns+0.0) / opts.episodes))\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "    # DISPLAY POST-LEARNING VALUES / Q-VALUES\n",
    "    if opts.agent == 'q' and not opts.manual:\n",
    "        try:\n",
    "            display.displayQValues(a, message = \"Q-VALUES AFTER \"+str(opts.episodes)+\" EPISODES\")\n",
    "            display.pause()\n",
    "            display.displayValues(a, message = \"VALUES AFTER \"+str(opts.episodes)+\" EPISODES\")\n",
    "            display.pause()\n",
    "        except KeyboardInterrupt:\n",
    "            sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-leadership",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
